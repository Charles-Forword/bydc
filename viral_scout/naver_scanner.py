import sys
import os

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ í´ë”ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (GitHub Actions í˜¸í™˜)
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import time
import ssl
import datetime
import json
import urllib.request
import urllib.parse
from urllib.parse import urlparse
import gspread
from oauth2client.service_account import ServiceAccountCredentials

def normalize_cafe_url(url):
    """
    ë„¤ì´ë²„ ì¹´í˜ URL ì •ê·œí™” (íŒŒë¼ë¯¸í„° ì œê±°)
    ì˜ˆ: https://cafe.naver.com/cafe_name/1234?art=... -> https://cafe.naver.com/cafe_name/1234
    """
    if not url:
        return ""
    
    try:
        parsed = urlparse(url)
        # ë„¤ì´ë²„ ì¹´í˜ ë„ë©”ì¸ì¸ì§€ í™•ì¸
        if "cafe.naver.com" in parsed.netloc:
            # pathê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°í•˜ê³  ë°˜í™˜
            if parsed.path and parsed.path != "/":
                return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        
        return url
    except:
        return url

# .env íŒŒì¼ ìë™ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenv ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì‚¬ìš©

# macOS SSL ì¸ì¦ì„œ ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ íŒ¨ì¹˜
ssl._create_default_https_context = ssl._create_unverified_context

from config import (
    NAVER_CLIENT_ID, NAVER_CLIENT_SECRET, 
    SEARCH_KEYWORDS, DISPLAY_COUNT, SORT_MODE,
    GOOGLE_SHEET_URL, SERVICE_ACCOUNT_FILE,
    BLOG_SHEET_NAME, CAFE_SHEET_NAME,
    TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID,
    EXCLUDE_KEYWORDS, REQUIRED_KEYWORDS, USE_AI_FILTER, OPENAI_API_KEY,
    ENABLE_CONTENT_SCRAPING, ENABLE_AI_ANALYSIS, ANALYZE_ALL,
    ENABLE_CAFE_CRAWLING, CAFE_MAX_POSTS, PRIORITIZE_QUESTIONS, FILTER_SPONSORED, ANALYZE_COMMENTS,
    AI_PROVIDER, GEMINI_API_KEY
)


def scrape_blog_content(url):
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ í¬ë¡¤ë§ (ì¬ì‹œë„ í¬í•¨)"""
    if not ENABLE_CONTENT_SCRAPING:
        return ""
    
    max_retries = 2
    for attempt in range(max_retries):
        try:
            from bs4 import BeautifulSoup
            import requests
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            content = soup.select_one('.se-main-container')
            if content:
                text = content.get_text(strip=True, separator=' ')[:2000]
                if len(text) > 100:
                    return text
            
            content = soup.select_one('#postViewArea')
            if content:
                text = content.get_text(strip=True, separator=' ')[:2000]
                if len(text) > 100:
                    return text
            
            paragraphs = soup.find_all(['p', 'div'], class_=lambda x: x and 'se-text' in x)
            if paragraphs:
                text = ' '.join([p.get_text(strip=True) for p in paragraphs])[:2000]
                if len(text) > 100:
                    return text
            
            if attempt < max_retries - 1:
                time.sleep(1)
                continue
            return "(ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨)"
            
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)
                continue
            print(f"      âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)[:50]}")
            return "(ë³¸ë¬¸ ì—†ìŒ)"
    
    return "(ë³¸ë¬¸ ì—†ìŒ)"


def is_blacklisted(title):
    """ì œì™¸ í‚¤ì›Œë“œê°€ ì œëª©ì— ìˆëŠ”ì§€ í™•ì¸"""
    for keyword in EXCLUDE_KEYWORDS:
        if keyword in title:
            return True
    return False

def has_required_keyword(title):
    """í•„ìˆ˜ í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ì œëª©ì— ìˆëŠ”ì§€ í™•ì¸"""
    for keyword in REQUIRED_KEYWORDS:
        if keyword in title:
            return True
    return False

def check_relevance_with_ai(title, description):
    """AIë¥¼ ì‚¬ìš©í•´ ë°˜ë ¤ë™ë¬¼ ì‚¬ë£Œ ê´€ë ¨ ê¸€ì¸ì§€ íŒë‹¨"""
    if not USE_AI_FILTER or not OPENAI_API_KEY:
        return True
    
    try:
        import requests
        
        prompt = f"""ë‹¤ìŒ ë¸”ë¡œê·¸ ê¸€ì´ "ë°˜ë ¤ë™ë¬¼(ê°•ì•„ì§€/ê³ ì–‘ì´) ì‚¬ë£Œ, ê°„ì‹, ì˜ì–‘ì œ" ê´€ë ¨ ë‚´ìš©ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
ì‚¬ëŒì´ ë¨¹ëŠ” ìŒì‹, í•œì‹ ë ˆì‹œí”¼, ë§›ì§‘, ì¸í…Œë¦¬ì–´ ë“±ì€ ê´€ë ¨ ì—†ìŠµë‹ˆë‹¤.

ì œëª©: {title}
ìš”ì•½: {description}

ë‹µë³€ì€ "YES" ë˜ëŠ” "NO"ë¡œë§Œ í•´ì£¼ì„¸ìš”."""

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {OPENAI_API_KEY}"
        }
        
        data = {
            "model": "gpt-4o-mini",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1,
            "max_tokens": 10
        }
        
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            answer = result['choices'][0]['message']['content'].strip().upper()
            return "YES" in answer
        else:
            return True
            
    except Exception as e:
        return True


def clean_ai_text(text):
    """AI ì‘ë‹µì—ì„œ ë§ˆí¬ë‹¤ìš´/ì´ëª¨ì§€ ì œê±°"""
    import re
    if not text:
        return ""
    text = text.replace("**", "").replace("*", "")
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # Emoticons
        u"\U0001F300-\U0001F5FF"  # Misc Symbols and Pictographs
        u"\U0001F680-\U0001F6FF"  # Transport and Map
        u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
        u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
        u"\U00002702-\U000027B0"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub('', text)
    return re.sub(r'\s+', ' ', text).strip()


def analyze_content_with_ai(title, content):
    """AIë¡œ ë¸”ë¡œê·¸ ë³¸ë¬¸ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
    if not ENABLE_AI_ANALYSIS:
        return {"ìš”ì•½": "", "ì£¼ìš”ë‚´ìš©": "", "ê²½ìŸì‚¬ì–¸ê¸‰": "", "ê°ì„±": "", "ì•¡ì…˜í¬ì¸íŠ¸": ""}
    
    if AI_PROVIDER == "gemini" and not GEMINI_API_KEY:
        return {"ìš”ì•½": "", "ì£¼ìš”ë‚´ìš©": "", "ê²½ìŸì‚¬ì–¸ê¸‰": "", "ê°ì„±": "", "ì•¡ì…˜í¬ì¸íŠ¸": ""}
    elif AI_PROVIDER == "openai" and not OPENAI_API_KEY:
        return {"ìš”ì•½": "", "ì£¼ìš”ë‚´ìš©": "", "ê²½ìŸì‚¬ì–¸ê¸‰": "", "ê°ì„±": "", "ì•¡ì…˜í¬ì¸íŠ¸": ""}
    
    if not ANALYZE_ALL and "ë³´ì–‘ëŒ€ì²©" not in title and "ë³´ì–‘ëŒ€ì²©" not in content:
        return {"ìš”ì•½": "", "ì£¼ìš”ë‚´ìš©": "", "ê²½ìŸì‚¬ì–¸ê¸‰": "", "ê°ì„±": "", "ì•¡ì…˜í¬ì¸íŠ¸": ""}
    
    try:
        import requests
        import json as json_module
        
        prompt = f"""ë°˜ë ¤ë™ë¬¼ ì‚¬ë£Œ ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë³¸ë¬¸: {content[:1500]}

ê·œì¹™:
1. ì´ ê¸€ì´ "ê°•ì•„ì§€" ë˜ëŠ” "ê³ ì–‘ì´"ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ê¸€ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”. (ì†Œë¼ê²Œ, í–„ìŠ¤í„°, ì‚¬ëŒ ìŒì‹ ë“±ì€ False)
2. ë§ˆí¬ë‹¤ìš´(**), ì´ëª¨ì§€, í•´ì‹œíƒœê·¸ ì‚¬ìš© ê¸ˆì§€
3. ê° í•„ë“œëŠ” ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ë˜, ë¬¸ì¥ì´ ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šë„ë¡ 'ë‹¤'ë¡œ ëë‚˜ëŠ” ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. (ê¶Œì¥ 100ì, ìµœëŒ€ 150ì)
4. í•´ë‹¹ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì‘ì„±

ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ (ë‹¤ë¥¸ ë§ ì—†ì´ JSONë§Œ):
{{
  "ë°˜ë ¤ë™ë¬¼ê´€ë ¨": true ë˜ëŠ” false,
  "ìš”ì•½": "í•µì‹¬ ë‚´ìš© 3-4ë¬¸ì¥ ìš”ì•½ (100~150ì ë‚´ì™¸ ìì—°ìŠ¤ëŸ¬ìš´ ë§¤ë“­ì§“ê¸°)",
  "ì£¼ìš”ë‚´ìš©": "ì–¸ê¸‰ëœ ì œí’ˆ íŠ¹ì§•ì´ë‚˜ íš¨ê³¼",
  "ê²½ìŸì‚¬ì–¸ê¸‰": "ì–¸ê¸‰ëœ ê²½ìŸ ë¸Œëœë“œëª…ë§Œ (ì—†ìœ¼ë©´ ë¹ˆì¹¸)",
  "ê°ì„±": "ê¸ì •/ì¤‘ë¦½/ë¶€ì • ì¤‘ í•˜ë‚˜",
  "ì•¡ì…˜í¬ì¸íŠ¸": "ê°œì„  ì œì•ˆì‚¬í•­"
}}"""

        if AI_PROVIDER == "gemini":
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"

            data = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {"temperature": 0.2, "maxOutputTokens": 600}
            }
            response = requests.post(url, json=data, timeout=15)

            if response.status_code == 200:
                result = response.json()
                ai_response = result['candidates'][0]['content']['parts'][0]['text'].strip()
            else:
                print(f"      âš ï¸ Gemini ì‹¤íŒ¨ ({response.status_code})")
                return {"ë°˜ë ¤ë™ë¬¼ê´€ë ¨": True, "ìš”ì•½": "", "ì£¼ìš”ë‚´ìš©": "", "ê²½ìŸì‚¬ì–¸ê¸‰": "", "ê°ì„±": "", "ì•¡ì…˜í¬ì¸íŠ¸": ""}
        
        else:
            headers = {"Content-Type": "application/json", "Authorization": f"Bearer {OPENAI_API_KEY}"}
            data = {
                "model": "gpt-4o-mini",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.2,
                "max_tokens": 600
            }
            response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=data, timeout=15)
            
            if response.status_code == 200:
                result = response.json()
                ai_response = result['choices'][0]['message']['content'].strip()
            else:
                print(f"      âš ï¸ OpenAI ì‹¤íŒ¨ ({response.status_code})")
                return {"ë°˜ë ¤ë™ë¬¼ê´€ë ¨": True, "ìš”ì•½": "", "ì£¼ìš”ë‚´ìš©": "", "ê²½ìŸì‚¬ì–¸ê¸‰": "", "ê°ì„±": "", "ì•¡ì…˜í¬ì¸íŠ¸": ""}
        
        # JSON íŒŒì‹±
        try:
            # ë””ë²„ê¹…: AI ì›ë³¸ ì‘ë‹µ ì¶œë ¥ (ì²˜ìŒ 200ì)
            print(f"      ğŸ“ AI ì›ë³¸ ì‘ë‹µ: {ai_response[:200]}...")
            
            if "```" in ai_response:
                ai_response = ai_response.split("```")[1]
                if ai_response.startswith("json"):
                    ai_response = ai_response[4:]
            
            analysis = json_module.loads(ai_response)
            # ê° í•„ë“œì—ì„œ ë§ˆí¬ë‹¤ìš´/ì´ëª¨ì§€ ì œê±°
            for key in analysis:
                if isinstance(analysis[key], str):
                    analysis[key] = clean_ai_text(analysis[key])
            
            # ê¸°ë³¸ê°’ True ì²˜ë¦¬ (í•„ë“œê°€ ì—†ì„ ê²½ìš°)
            if "ë°˜ë ¤ë™ë¬¼ê´€ë ¨" not in analysis:
                analysis["ë°˜ë ¤ë™ë¬¼ê´€ë ¨"] = True
                
            return analysis
        except Exception as parse_err:
            print(f"      âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {parse_err}")
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ê°’ ë°˜í™˜ (ì´ìƒí•œ í…ìŠ¤íŠ¸ ì €ì¥ ë°©ì§€)
            return {"ë°˜ë ¤ë™ë¬¼ê´€ë ¨": True, "ìš”ì•½": "", "ì£¼ìš”ë‚´ìš©": "", "ê²½ìŸì‚¬ì–¸ê¸‰": "", "ê°ì„±": "", "ì•¡ì…˜í¬ì¸íŠ¸": ""}
            
    except Exception as e:
        print(f"      âš ï¸ AI ì˜¤ë¥˜: {str(e)[:50]}")
        return {"ë°˜ë ¤ë™ë¬¼ê´€ë ¨": True, "ìš”ì•½": "", "ì£¼ìš”ë‚´ìš©": "", "ê²½ìŸì‚¬ì–¸ê¸‰": "", "ê°ì„±": "", "ì•¡ì…˜í¬ì¸íŠ¸": ""}


def send_telegram_message(message):
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ë°œì†¡"""
    try:
        import requests
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
        data = {"chat_id": TELEGRAM_CHAT_ID, "text": message, "parse_mode": "HTML"}
        response = requests.post(url, data=data, timeout=10)
        
        if response.status_code == 200:
            print("âœ… í…”ë ˆê·¸ë¨ ë°œì†¡ ì„±ê³µ")
        else:
            print(f"âŒ í…”ë ˆê·¸ë¨ ë°œì†¡ ì‹¤íŒ¨: {response.status_code}")
    except Exception as e:
        print(f"âŒ í…”ë ˆê·¸ë¨ ë°œì†¡ ì˜¤ë¥˜: {e}")

def format_date(date_str):
    """ë‚ ì§œ í˜•ì‹ ë³€í™˜"""
    try:
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
    except:
        return date_str

def get_existing_links(sheet, link_column_index):
    """
    êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ê¸°ì¡´ ë§í¬ ëª©ë¡ ì¶”ì¶œ (ì¤‘ë³µ ì²´í¬ìš©)
    
    Args:
        sheet: gspread ì‹œíŠ¸ ê°ì²´
        link_column_index: ë§í¬ê°€ ìˆëŠ” ì—´ ì¸ë±ìŠ¤ (0-based)
    
    Returns:
        set: ê¸°ì¡´ ë§í¬ ì§‘í•© (ì •ê·œí™”ë¨)
    """
    try:
        all_values = sheet.get_all_values()
        if len(all_values) <= 1:
            return set()
        
        links = set()
        for row in all_values[1:]:
            if len(row) > link_column_index and row[link_column_index]:
                # URL ì •ê·œí™”í•˜ì—¬ ì €ì¥ (ë¹„êµ ì •í™•ë„ í–¥ìƒ)
                links.add(normalize_cafe_url(row[link_column_index]))
        return links
    except Exception as e:
        print(f"      âš ï¸ ê¸°ì¡´ ë§í¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return set()


def load_keywords_from_sheet(spreadsheet):
    """
    [ê²€ìƒ‰ì„¤ì •] íƒ­ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¡œë“œ
    
    ì‹œíŠ¸ êµ¬ì¡°: 
    - A1: í—¤ë” (ì˜ˆ: "ê²€ìƒ‰í‚¤ì›Œë“œ") - ìŠ¤í‚µë¨
    - A2ë¶€í„°: ì‹¤ì œ í‚¤ì›Œë“œ ë‚˜ì—´
    
    Returns:
        list: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    try:
        settings_sheet = spreadsheet.worksheet("ê²€ìƒ‰ì„¤ì •")
        # Aì—´ ì „ì²´ ì½ê¸°
        all_keywords = settings_sheet.col_values(1)
        # 1í–‰(í—¤ë”) ì œì™¸í•˜ê³  ë¹ˆ ê°’ ì œê±°
        keywords = [k.strip() for k in all_keywords[1:] if k.strip()]
        
        if keywords:
            print(f"ğŸ“ [ê²€ìƒ‰ì„¤ì •] íƒ­ì—ì„œ {len(keywords)}ê°œ í‚¤ì›Œë“œ ë¡œë“œ")
            for i, kw in enumerate(keywords[:5]):
                print(f"   {i+1}. {kw}")
            if len(keywords) > 5:
                print(f"   ... ì™¸ {len(keywords)-5}ê°œ")
            return keywords
        else:
            print("âš ï¸ [ê²€ìƒ‰ì„¤ì •] íƒ­ì— í‚¤ì›Œë“œ ì—†ìŒ, config.py ê¸°ë³¸ê°’ ì‚¬ìš©")
            return None
    except Exception as e:
        print(f"âš ï¸ [ê²€ìƒ‰ì„¤ì •] íƒ­ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        print("   config.py ê¸°ë³¸ê°’ ì‚¬ìš©")
        return None

def filter_new_posts(posts, existing_links, source_type="ì¹´í˜"):
    """
    ì‹ ê·œ ê²Œì‹œê¸€ë§Œ í•„í„°ë§ (ì¤‘ë³µ ì œì™¸)
    
    Args:
        posts: ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
        existing_links: ê¸°ì¡´ ë§í¬ ì§‘í•©
        source_type: "ë¸”ë¡œê·¸" ë˜ëŠ” "ì¹´í˜"
    
    Returns:
        list: ì¤‘ë³µ ì œì™¸ëœ ì‹ ê·œ ê²Œì‹œê¸€
    """
    new_posts = []
    for p in posts:
        raw_link = p.get('link')
        # ë§í¬ ì •ê·œí™” (íŒŒë¼ë¯¸í„° ì œê±° ë“±)
        normalized_link = normalize_cafe_url(raw_link)
        
        if normalized_link not in existing_links:
            # ì €ì¥ë  ë°ì´í„°ë„ ì •ê·œí™”ëœ ë§í¬ë¡œ ì—…ë°ì´íŠ¸
            p['link'] = normalized_link
            new_posts.append(p)
            
    duplicates = len(posts) - len(new_posts)
    
    if duplicates > 0:
        print(f"   ğŸ”„ [{source_type}] ì¤‘ë³µ {duplicates}ê±´ ì œì™¸, ì‹ ê·œ {len(new_posts)}ê±´")
    
    return new_posts

def init_google_sheets():
    """êµ¬ê¸€ ì‹œíŠ¸ ì´ˆê¸°í™” (ë¸”ë¡œê·¸ + ì¹´í˜ ë³„ë„ ì‹œíŠ¸)
    
    Returns:
        tuple: (blog_sheet, cafe_sheet, spreadsheet)
    """
    try:
        if os.environ.get("GITHUB_ACTIONS"):
            print("â„¹ï¸ GitHub Env: Creating service_account.json from secret")
            json_content = os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON", "")
            
            if not json_content:
                print("âŒ Error: GOOGLE_SERVICE_ACCOUNT_JSON secret is empty!")
                raise ValueError("GOOGLE_SERVICE_ACCOUNT_JSON secret is missing")
            
            # Base64 ë””ì½”ë”© ì‹œë„ (JSONì´ ì•„ë‹ˆê±°ë‚˜ '{'ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ Base64ë¡œ ê°„ì£¼)
            if not json_content.strip().startswith("{"):
                try:
                    import base64
                    decoded_bytes = base64.b64decode(json_content)
                    json_content = decoded_bytes.decode('utf-8')
                    print("â„¹ï¸ Base64 encoded secret detected and decoded.")
                except Exception as e:
                    print(f"âš ï¸ Base64 decode failed, using raw content: {e}")
            
            with open("service_account.json", "w") as f:
                f.write(json_content)
            
            if os.path.exists("service_account.json"):
                file_size = os.path.getsize('service_account.json')
                print(f"âœ… service_account.json created (size: {file_size} bytes)")
                
                # JSON ìœ íš¨ì„± ê²€ì‚¬ ë° í•„ìˆ˜ í‚¤ ì²´í¬
                try:
                    import json as json_module
                    with open("service_account.json", "r") as check_file:
                        sa_data = json_module.load(check_file)
                    
                    required_keys = ["type", "project_id", "private_key", "client_email", "client_id"]
                    missing_keys = [k for k in required_keys if k not in sa_data]
                    
                    if missing_keys:
                        print(f"âŒ service_account.jsonì— í•„ìˆ˜ í‚¤ ëˆ„ë½: {missing_keys}")
                        print(f"   í˜„ì¬ í‚¤: {list(sa_data.keys())}")
                    else:
                        print(f"âœ… service_account.json í•„ìˆ˜ í‚¤ í™•ì¸ ì™„ë£Œ")
                        print(f"   client_email: {sa_data.get('client_email', 'N/A')}")
                except Exception as json_err:
                    print(f"âŒ service_account.json JSON íŒŒì‹± ì‹¤íŒ¨: {json_err}")
            else:
                print("âŒ Failed to create service_account.json")

        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_FILE, scope)
        client = gspread.authorize(creds)
        spreadsheet = client.open_by_url(GOOGLE_SHEET_URL)
        
        # ë¸”ë¡œê·¸ ì‹œíŠ¸ (ê¸°ì¡´)
        try:
            blog_sheet = spreadsheet.worksheet(BLOG_SHEET_NAME)
        except:
            blog_sheet = spreadsheet.sheet1
        
        if not blog_sheet.row_values(1):
            blog_sheet.append_row(["ìˆ˜ì§‘ì¼ì‹œ", "í‚¤ì›Œë“œ", "ì œëª©", "ë‚ ì§œ", "ë§í¬", "ìƒíƒœ", "ìš”ì•½", "ì£¼ìš”ë‚´ìš©", "ê²½ìŸì‚¬ì–¸ê¸‰", "ê°ì„±", "ì•¡ì…˜í¬ì¸íŠ¸"])
            print(f"âœ… ë¸”ë¡œê·¸ ì‹œíŠ¸ '{BLOG_SHEET_NAME}' í—¤ë” ì¶”ê°€")
        
        # ì¹´í˜ ì‹œíŠ¸ (ì‹ ê·œ)
        try:
            cafe_sheet = spreadsheet.worksheet(CAFE_SHEET_NAME)
        except:
            print(f"ğŸ“‹ '{CAFE_SHEET_NAME}' ì‹œíŠ¸ ìƒì„± ì¤‘...")
            cafe_sheet = spreadsheet.add_worksheet(title=CAFE_SHEET_NAME, rows=1000, cols=20)
        
        if not cafe_sheet.row_values(1):
            cafe_sheet.append_row([
                "ìˆ˜ì§‘ì¼ì‹œ", "í‚¤ì›Œë“œ", "ì¹´í˜ëª…", "ì œëª©", "ë‚ ì§œ", "ë§í¬",
                "ë³¸ë¬¸ë‚´ìš©ìš”ì•½", "ëŒ“ê¸€ìˆ˜", "í•µì‹¬ì—°ê´€í‚¤ì›Œë“œ", "ê²½ìŸì‚¬ì–¸ê¸‰"
            ])
            print(f"âœ… ì¹´í˜ ì‹œíŠ¸ '{CAFE_SHEET_NAME}' í—¤ë” ì¶”ê°€")
            
        return blog_sheet, cafe_sheet, spreadsheet
    except Exception as e:
        print(f"âŒ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")
        return None, None, None

def search_naver_blog(query):
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
    encText = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/blog?query={encText}&display={DISPLAY_COUNT}&sort={SORT_MODE}"
    
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    
    try:
        response = urllib.request.urlopen(request)
        if response.getcode() == 200:
            return json.loads(response.read().decode('utf-8'))
    except Exception as e:
        print(f"   âŒ API ì˜¤ë¥˜: {e}")
    return None

def main():
    print("ğŸš€ Viral Scout: Naver & Google Sheet Scanning Started...")
    
    # API í‚¤ ì²´í¬
    if ENABLE_AI_ANALYSIS:
        if AI_PROVIDER == "gemini":
            print(f"âœ… AI Provider: Gemini" + (" (API í‚¤ í™•ì¸ë¨)" if GEMINI_API_KEY else " âš ï¸ API í‚¤ ì—†ìŒ"))
        elif AI_PROVIDER == "openai":
            print(f"âœ… AI Provider: OpenAI" + (" (API í‚¤ í™•ì¸ë¨)" if OPENAI_API_KEY else " âš ï¸ API í‚¤ ì—†ìŒ"))
    
    blog_sheet, cafe_sheet, spreadsheet = init_google_sheets()
    if not blog_sheet:
        print("âŒ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨ë¡œ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(1)  # GitHub Actionsì—ì„œ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬ë˜ë„ë¡ Exit Code 1 ë°˜í™˜

    print("âœ… ì‹œíŠ¸ ì—°ê²° ì„±ê³µ!")

    # [ê²€ìƒ‰ì„¤ì •] íƒ­ì—ì„œ í‚¤ì›Œë“œ ë¡œë“œ (ì—†ìœ¼ë©´ config.py ê¸°ë³¸ê°’)
    search_keywords = load_keywords_from_sheet(spreadsheet) or SEARCH_KEYWORDS
    
    if not search_keywords:
        print("âŒ ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(1)
        
    print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")

    # KST (UTC+9) ì„¤ì •
    kst = datetime.timezone(datetime.timedelta(hours=9))
    today_str = datetime.datetime.now(kst).strftime("%Y-%m-%d %H:%M:%S")
    blog_rows = []  # ë¸”ë¡œê·¸ ë°ì´í„°
    cafe_rows = []  # ì¹´í˜ ë°ì´í„°
    briefing_lines = []

    # Phase 2: ë¸”ë¡œê·¸ ê²€ìƒ‰ (í™œì„±í™”)
    # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ ê¸°ì¡´ ë§í¬ ë¡œë“œ (Eì—´=ë§í¬, ì¸ë±ìŠ¤ 4)
    existing_blog_links = get_existing_links(blog_sheet, 4)
    print(f"\nğŸ“ Phase 2: ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹œì‘...")
    print(f"   ğŸ“‹ ê¸°ì¡´ ë¸”ë¡œê·¸ ê¸€: {len(existing_blog_links)}ê±´")
    
    for keyword in search_keywords:
        print(f"\nğŸ” ê²€ìƒ‰ì–´: '{keyword}'")
        result = search_naver_blog(keyword)
        
        keyword_count = 0
        if result and 'items' in result:
            items = result['items']
            if not items:
                print("   (ê²°ê³¼ ì—†ìŒ)")
                continue

            for item in items:
                title = item['title'].replace('<b>', '').replace('</b>', '').replace('&quot;', '"')
                link = item['link']
                postdate = format_date(item['postdate'])
                description = item.get('description', '').replace('<b>', '').replace('</b>', '').replace('&quot;', '"')
                
                # ì¤‘ë³µ ì²´í¬
                if link in existing_blog_links:
                    continue
                
                if is_blacklisted(title):
                    print(f"   ğŸš« ì œì™¸(ë¸”ë™ë¦¬ìŠ¤íŠ¸): {title[:40]}")
                    continue
                
                if not has_required_keyword(title):
                    print(f"   ğŸš« ì œì™¸(í•„ìˆ˜í‚¤ì›Œë“œ): {title[:40]}")
                    continue
                
                # descriptionì„ ë³¸ë¬¸ìœ¼ë¡œ ì‚¬ìš© (150ì ë¯¸ë¦¬ë³´ê¸°, í¬ë¡¤ë§ë³´ë‹¤ ì•ˆì •ì )
                content = description
                
                print(f"   ğŸ§  AI ë¶„ì„ ({len(content)}ì)...")
                analysis = analyze_content_with_ai(title, content)
                
                # AIê°€ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì—†ë‹¤ê³  íŒë‹¨í•˜ë©´ ì œì™¸
                if not analysis.get("ë°˜ë ¤ë™ë¬¼ê´€ë ¨", True):
                    print(f"   ğŸš« ì œì™¸(AIíŒë‹¨): {title[:40]}")
                    continue

                # ë¸”ë¡œê·¸ ë°ì´í„° (ê°„ê²° í˜•ì‹)
                row_data = [
                    today_str, keyword, title, postdate, link, "ì‹ ê·œ",
                    analysis.get("ìš”ì•½", ""),
                    analysis.get("ì£¼ìš”ë‚´ìš©", ""),
                    analysis.get("ê²½ìŸì‚¬ì–¸ê¸‰", ""),
                    analysis.get("ê°ì„±", ""),
                    analysis.get("ì•¡ì…˜í¬ì¸íŠ¸", "")
                ]
                
                blog_rows.append(row_data)
                print(f"   âœ… ì¤€ë¹„: {title[:40]}")
                if analysis.get("ìš”ì•½"):
                    print(f"      ğŸ’¡ {analysis['ìš”ì•½'][:50]}...")
                
                keyword_count += 1
                if keyword_count <= 2:
                    briefing_lines.append(f"- [{keyword}] {title}")
                
                time.sleep(0.5)

        else:
            print("   (API ì‹¤íŒ¨)")
        
        time.sleep(1)
    
    # Phase 3: ì¹´í˜ í¬ë¡¤ë§
    if ENABLE_CAFE_CRAWLING:
        try:
            from cafe_scanner import search_cafe_posts
            from content_filters import (
                detect_sponsored_content,
                is_genuine_question,
                analyze_comments_batch,
                extract_keywords_hybrid,
                extract_competitors
            )
            
            print(f"\n\nğŸ¢ Phase 3: ì¹´í˜ ê²€ìƒ‰ ì‹œì‘...")
            cafe_briefing = []
            
            # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ ê¸°ì¡´ ë§í¬ ë¡œë“œ (Fì—´=ë§í¬, ì¸ë±ìŠ¤ 5)
            existing_cafe_links = get_existing_links(cafe_sheet, 5)
            print(f"   ğŸ“‹ ê¸°ì¡´ ì¹´í˜ ê¸€: {len(existing_cafe_links)}ê±´")
            
            for keyword in search_keywords:
                print(f"\nğŸ” [ì¹´í˜] '{keyword}'")
                cafe_posts = search_cafe_posts(keyword, max_posts=CAFE_MAX_POSTS)
                
                # ì¤‘ë³µ ì œì™¸
                new_posts = filter_new_posts(cafe_posts, existing_cafe_links, "ì¹´í˜")
                
                for post in new_posts:
                    # 1. ëŒ“ê¸€ ìˆ˜ í™•ì¸
                    comment_count = post.get('comment_count', 0)
                    is_question = is_genuine_question(post['title'], post['content'])
                    
                    # ëŒ“ê¸€ 0ê°œì¸ ê¸€ì€ ì§ˆë¬¸í˜•íƒœê°€ ì•„ë‹ˆë©´ ì œì™¸
                    if comment_count == 0 and not is_question:
                        print(f"   ğŸš« ëŒ“ê¸€ì—†ìŒ(ë¹„ì§ˆë¬¸): {post['title'][:40]}")
                        continue
                    
                    # 2. í˜‘ì°¬ í•„í„°ë§ (ì„ íƒ)
                    if FILTER_SPONSORED:
                        if detect_sponsored_content(post['title'], post['content']):
                            print(f"   ğŸš« í˜‘ì°¬ê¸€ ì œì™¸: {post['title'][:40]}")
                            continue
                    
                    # 3. AI ìš”ì•½
                    print(f"   ğŸ§  AI ìš”ì•½ ì¤‘...")
                    from content_filters import analyze_cafe_content
                    ai_analysis = analyze_cafe_content(post['title'], post['content'])
                    
                    # AIê°€ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì—†ë‹¤ê³  íŒë‹¨í•˜ë©´ ì œì™¸
                    if not ai_analysis.get("ë°˜ë ¤ë™ë¬¼ê´€ë ¨", True):
                        print(f"   ğŸš« ì œì™¸(AIíŒë‹¨): {post['title'][:40]}")
                        continue
                    
                    # 4. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (Iì—´: ì§€ì • í‚¤ì›Œë“œë§Œ)
                    keywords_str = extract_keywords_hybrid(post['title'], post['content'])
                    
                    # 5. ê²½ìŸì‚¬ ì–¸ê¸‰ ì¶”ì¶œ (Jì—´: ì§€ì • ê²½ìŸì‚¬ë§Œ)
                    competitors_str = extract_competitors(post['title'], post['content'])
                    
                    # ì¹´í˜ ë°ì´í„° (ì´ë¯¸ì§€ ì—´ ì œê±°)
                    # A: ìˆ˜ì§‘ì¼ì‹œ, B: í‚¤ì›Œë“œ, C: ì¹´í˜ëª…
                    # D: ì œëª©, E: ë‚ ì§œ, F: ë§í¬
                    # G: ë³¸ë¬¸ë‚´ìš©ìš”ì•½ (AI ìš”ì•½, 100ì)
                    # H: ëŒ“ê¸€ìˆ˜
                    # I: í•µì‹¬ì—°ê´€í‚¤ì›Œë“œ (ì§€ì • í‚¤ì›Œë“œì—ì„œ ë§¤ì¹­)
                    # J: ê²½ìŸì‚¬ì–¸ê¸‰ (ì§€ì • ê²½ìŸì‚¬ì—ì„œ ë§¤ì¹­)
                    
                    row_data = [
                        today_str,                                  # A: ìˆ˜ì§‘ì¼ì‹œ
                        keyword,                                    # B: í‚¤ì›Œë“œ
                        post['cafe_name'],                          # C: ì¹´í˜ëª…
                        post['title'],                              # D: ì œëª©
                        post['date'],                               # E: ë‚ ì§œ
                        post['link'],                               # F: ë§í¬
                        ai_analysis.get("ìš”ì•½", "")[:100],          # G: ë³¸ë¬¸ë‚´ìš©ìš”ì•½ (100ì)
                        comment_count,                              # H: ëŒ“ê¸€ìˆ˜
                        keywords_str,                               # I: í•µì‹¬ì—°ê´€í‚¤ì›Œë“œ
                        competitors_str                             # J: ê²½ìŸì‚¬ì–¸ê¸‰
                    ]
                    
                    cafe_rows.append(row_data)
                    print(f"   âœ… ì¤€ë¹„: {post['title'][:40]}")
                    if competitors_str:
                        print(f"      ğŸ† ê²½ìŸì‚¬ ì–¸ê¸‰: {competitors_str}")
                    
                    if is_question:
                        cafe_briefing.append(f"- [ì§ˆë¬¸/{post['cafe_name']}] {post['title'][:40]}")
                    
                    time.sleep(0.5)
                
                time.sleep(2)  # ì¹´í˜ ê°„ delay
            
            if cafe_briefing:
                briefing_lines.extend(cafe_briefing[:5])
        
        except Exception as e:
            print(f"\nâš ï¸ ì¹´í˜ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    # ë¶„ë¦¬ ì €ì¥
    total_count = 0
    
    # ë¸”ë¡œê·¸ ë°ì´í„° ì €ì¥
    if blog_rows:
        print(f"\nğŸ“š ë¸”ë¡œê·¸ {len(blog_rows)}ê±´ ì €ì¥ ì¤‘...")
        try:
            blog_sheet.append_rows(blog_rows, value_input_option='RAW')
            print(f"âœ… ë¸”ë¡œê·¸ {len(blog_rows)}ê±´ ì €ì¥ ì™„ë£Œ!")
            total_count += len(blog_rows)
        except Exception as e:
            print(f"âŒ ë¸”ë¡œê·¸ ë°°ì¹˜ ì‹¤íŒ¨: {e}")
    
    # ì¹´í˜ ë°ì´í„° ì €ì¥
    if cafe_rows:
        print(f"\nğŸª ì¹´í˜ {len(cafe_rows)}ê±´ ì €ì¥ ì¤‘...")
        try:
            # USER_ENTEREDë¡œ ë³€ê²½í•˜ì—¬ IMAGE í•¨ìˆ˜ê°€ ì‘ë™í•˜ë„ë¡ í•¨
            cafe_sheet.append_rows(cafe_rows, value_input_option='USER_ENTERED')
            print(f"âœ… ì¹´í˜ {len(cafe_rows)}ê±´ ì €ì¥ ì™„ë£Œ!")
            total_count += len(cafe_rows)
        except Exception as e:
            print(f"âŒ ì¹´í˜ ë°°ì¹˜ ì‹¤íŒ¨: {e}")
    print(f"\nğŸ‰ ì´ {total_count}ê±´ ì €ì¥ ì™„ë£Œ!")
    
    # í…”ë ˆê·¸ë¨ ë³´ê³  ë©”ì‹œì§€ ìƒì„±
    blog_new_count = len(blog_rows)
    cafe_new_count = len(cafe_rows)
    
    # ëˆ„ì  ê°œìˆ˜ ê³„ì‚° (ê¸°ì¡´ + ì‹ ê·œ)
    blog_total = len(existing_blog_links) + blog_new_count
    cafe_total = len(existing_cafe_links) + cafe_new_count if ENABLE_CAFE_CRAWLING else 0
    
    if total_count > 0:
        # ì œëª© 30ì ìë¥´ê¸° í•¨ìˆ˜
        def truncate_title(title, max_len=30):
            return title[:max_len] + "..." if len(title) > max_len else title
        
        msg = f"ì˜¤ëŠ˜ ì´ {total_count}ê°œì˜ ê¸€ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤!\n\n"
        msg += f"ë¸”ë¡œê·¸ : +{blog_new_count}/{blog_total}\n"
        msg += f"ì¹´í˜ : +{cafe_new_count}/{cafe_total}\n\n"
        
        # ë¸”ë¡œê·¸ ëª©ë¡ (ìµœëŒ€ 5ê°œ)
        if blog_rows:
            msg += "ã€ë¸”ë¡œê·¸ã€‘\n"
            for row in blog_rows[:5]:
                keyword = row[1]  # Bì—´: í‚¤ì›Œë“œ
                title = row[2]    # Cì—´: ì œëª©
                msg += f" - [{keyword}] {truncate_title(title)}\n"
            if len(blog_rows) > 5:
                msg += f" ... ì™¸ {len(blog_rows) - 5}ê°œ\n"
            msg += "\n"
        
        # ì¹´í˜ ëª©ë¡ (ìµœëŒ€ 5ê°œ)
        if cafe_rows:
            msg += "ã€ì¹´í˜ã€‘\n"
            for row in cafe_rows[:5]:
                keyword = row[1]  # Bì—´: í‚¤ì›Œë“œ
                title = row[3]    # Dì—´: ì œëª©
                msg += f" - [{keyword}] {truncate_title(title)}\n"
            if len(cafe_rows) > 5:
                msg += f" ... ì™¸ {len(cafe_rows) - 5}ê°œ\n"
            msg += "\n"
        
        msg += f"ğŸ‘‰ {GOOGLE_SHEET_URL}"
        send_telegram_message(msg)
    else:
        print("ì‹ ê·œ ë°ì´í„° ì—†ìŒ")

if __name__ == "__main__":
    main()
