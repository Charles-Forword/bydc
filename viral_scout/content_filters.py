"""
AI ê¸°ë°˜ ì½˜í…ì¸  í•„í„°
- í˜‘ì°¬/ê´‘ê³ ì„± ë¦¬ë·° ê°ì§€
- ì§„ì •í•œ ì§ˆë¬¸ê¸€ íŒë³„
- ëŒ“ê¸€ ê°ì„± ë¶„ì„
"""

import requests
import json
from config import AI_PROVIDER, GEMINI_API_KEY, OPENAI_API_KEY


# í˜‘ì°¬ ê°ì§€ í‚¤ì›Œë“œ
SPONSORED_KEYWORDS = [
    "í˜‘ì°¬", "ì§€ì›", "ì œê³µë°›", "ë¬´ìƒ", "ì²´í—˜ë‹¨", 
    "ì´ë²¤íŠ¸ ë‹¹ì²¨", "ë¦¬ë·°ì–´", "ì„œí¬í„°ì¦ˆ", "ì•°ë²„ì„œë”",
    "ì›ê³ ë£Œ", "ê´‘ê³ ", "PR", "í”„ë¡œëª¨ì…˜"
]

# ì§ˆë¬¸ íŒ¨í„´
QUESTION_PATTERNS = [
    "ì–´ë–¤ê°€ìš”", "ê´œì°®ë‚˜ìš”", "ì¶”ì²œ", "ì–´ë–»ê²Œ", "ì–´ë•Œìš”",
    "ë¨¹ì—¬ë„ ë ê¹Œìš”", "ê´œì°®ì„ê¹Œìš”", "ê³ ë¯¼", "ê¶ê¸ˆ",
    "ë¬¸ì˜", "ì§ˆë¬¸", "ì—¬ì­¤", "ë„ì™€ì£¼ì„¸ìš”"
]


def detect_sponsored_content(title, content):
    """
    í˜‘ì°¬/ê´‘ê³ ì„± ì½˜í…ì¸  ê°ì§€ (ì•½í™” ë²„ì „)
    
    ëª…ì‹œì  í˜‘ì°¬ í‚¤ì›Œë“œë§Œ ì²´í¬ (AI íŒë‹¨ ì œê±°)
    - ì´ìœ : AI íŒë‹¨ì€ ëŠë¦¬ê³  ì˜¤íƒ(false positive)ì´ ë§ìŒ
    - ì§„ì§œ ê³ ê° ë¦¬ë·°ë„ ê¸ì •ì ì´ë©´ í˜‘ì°¬ìœ¼ë¡œ ì˜¤ì¸ë¨
    
    Returns:
        bool: Trueë©´ ëª…í™•í•œ í˜‘ì°¬ê¸€ (í•„í„°ë§ ëŒ€ìƒ)
    """
    # ëª…ì‹œì  í˜‘ì°¬ í‚¤ì›Œë“œë§Œ í™•ì¸ (AI íŒë‹¨ ì œê±°)
    full_text = title + content[:500]  # ë³¸ë¬¸ ì „ì²´ ëŒ€ì‹  ì•ë¶€ë¶„ë§Œ ì²´í¬
    
    for keyword in SPONSORED_KEYWORDS:
        if keyword in full_text:
            return True
    
    # AI íŒë‹¨ ì œê±° - ë„ˆë¬´ ë§ì€ ì •ìƒ ë¦¬ë·°ë¥¼ ì°¨ë‹¨í•¨
    return False


def is_genuine_question(title, content):
    """
    ì§„ì§œ ì§ˆë¬¸ê¸€ì¸ì§€ íŒë‹¨
    
    Returns:
        bool: Trueë©´ ì§ˆë¬¸ê¸€ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    """
    # 1ë‹¨ê³„: íŒ¨í„´ í™•ì¸
    full_text = title + content[:200]
    
    # ì œëª©ì— ë¬¼ìŒí‘œ
    has_question_mark = "?" in title or "?" in title
    
    # ì§ˆë¬¸ íŒ¨í„´
    has_question_pattern = any(pattern in full_text for pattern in QUESTION_PATTERNS)
    
    if has_question_mark and has_question_pattern:
        return True
    
    # 2ë‹¨ê³„: AI íŒë‹¨ (ê²½ê³„ì„  ì¼€ì´ìŠ¤)
    if not has_question_mark or not GEMINI_API_KEY:
        return False
    
    try:
        prompt = f"""ë‹¤ìŒ ê¸€ì´ ì œí’ˆì— ëŒ€í•œ ì§„ì§œ ì§ˆë¬¸ê¸€ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”:

ì œëª©: {title}
ë³¸ë¬¸: {content[:300]}

ì§„ì§œ ì§ˆë¬¸ì´ë€:
- êµ¬ë§¤ ì „ ê³ ë¯¼/ë¬¸ì˜
- ì‚¬ìš© ê²½í—˜ ë¬¼ì–´ë´„
- ì¶”ì²œ ìš”ì²­

YES ë˜ëŠ” NOë¡œë§Œ ë‹µë³€:"""

        ai_response = call_ai_api(prompt, max_tokens=10)
        
        return "YES" in ai_response.upper()
    
    except:
        return False


def analyze_comment_sentiment(comment_text):
    """
    ëŒ“ê¸€ ê°ì„± ë¶„ì„
    
    Returns:
        str: "ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½"
    """
    if not GEMINI_API_KEY and not OPENAI_API_KEY:
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±
        positive_words = ["ì¢‹ì•„ìš”", "ë§Œì¡±", "ì¶”ì²œ", "ê´œì°®", "ì¢‹ë„¤ìš”", "êµ¿"]
        negative_words = ["ë³„ë¡œ", "ì‹¤ë§", "ì•ˆì¢‹", "ì„¤ì‚¬", "ì•ˆë§", "í›„íšŒ", "ìµœì•…"]
        
        pos_count = sum(1 for w in positive_words if w in comment_text)
        neg_count = sum(1 for w in negative_words if w in comment_text)
        
        if neg_count > pos_count:
            return "ë¶€ì •"
        elif pos_count > neg_count:
            return "ê¸ì •"
        else:
            return "ì¤‘ë¦½"
    
    try:
        prompt = f"""ë‹¤ìŒ ëŒ“ê¸€ì˜ ê°ì„±ì„ ë¶„ì„í•˜ì„¸ìš”:

"{comment_text}"

'ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½' ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€:"""

        ai_response = call_ai_api(prompt, max_tokens=10)
        
        if "ë¶€ì •" in ai_response:
            return "ë¶€ì •"
        elif "ê¸ì •" in ai_response:
            return "ê¸ì •"
        else:
            return "ì¤‘ë¦½"
    
    except:
        return "ì¤‘ë¦½"


def analyze_comments_batch(comments_list):
    """
    ëŒ“ê¸€ ëª©ë¡ ì¼ê´„ ë¶„ì„
    
    Returns:
        dict: ê°ì„± í†µê³„ + ì£¼ìš” ë¶€ì • ì˜ê²¬
    """
    if not comments_list:
        return {
            "ê¸ì •_ê°œìˆ˜": 0,
            "ë¶€ì •_ê°œìˆ˜": 0,
            "ì¤‘ë¦½_ê°œìˆ˜": 0,
            "ë¶€ì •_ì˜ˆì‹œ": [],
            "ì£¼ìš”_ë¶ˆë§Œ": ""
        }
    
    positive = []
    negative = []
    neutral = []
    
    for comment in comments_list:
        sentiment = analyze_comment_sentiment(comment['content'])
        
        comment['sentiment'] = sentiment
        
        if sentiment == "ê¸ì •":
            positive.append(comment)
        elif sentiment == "ë¶€ì •":
            negative.append(comment)
        else:
            neutral.append(comment)
    
    # ë¶€ì • ì˜ê²¬ ì£¼ìš” ì´ìŠˆ ì¶”ì¶œ
    key_issues = extract_key_issues(negative) if negative else ""
    
    return {
        "ê¸ì •_ê°œìˆ˜": len(positive),
        "ë¶€ì •_ê°œìˆ˜": len(negative),
        "ì¤‘ë¦½_ê°œìˆ˜": len(neutral),
        "ë¶€ì •_ì˜ˆì‹œ": [c['content'][:50] for c in negative[:3]],
        "ì£¼ìš”_ë¶ˆë§Œ": key_issues
    }


def extract_key_issues(negative_comments):
    """
    ë¶€ì • ëŒ“ê¸€ì—ì„œ ì£¼ìš” ì´ìŠˆ ì¶”ì¶œ
    """
    if not negative_comments or (not GEMINI_API_KEY and not OPENAI_API_KEY):
        return ""
    
    try:
        comments_text = "\n- ".join([c['content'][:100] for c in negative_comments[:5]])
        
        prompt = f"""ë‹¤ìŒ ë¶€ì •ì  ëŒ“ê¸€ë“¤ì˜ ê³µí†µ ë¶ˆë§Œì‚¬í•­ì„ í•œ ì¤„ë¡œ ìš”ì•½:

{comments_text}

í•µì‹¬ ì´ìŠˆë§Œ ê°„ë‹¨íˆ (ì˜ˆ: ì•ŒëŸ¬ì§€ ë°˜ì‘, ê¸°í˜¸ì„± ë‚®ìŒ):"""

        return call_ai_api(prompt, max_tokens=50)
    
    except:
        return ""


def call_ai_api(prompt, max_tokens=100):
    """AI API í˜¸ì¶œ (Gemini ë˜ëŠ” OpenAI)"""
    
    if AI_PROVIDER == "gemini" and GEMINI_API_KEY:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"
        
        data = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": 0.2,
                "maxOutputTokens": max_tokens
            }
        }
        
        response = requests.post(url, json=data, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            return result['candidates'][0]['content']['parts'][0]['text'].strip()
        else:
            raise Exception(f"Gemini API error: {response.status_code}")
    
    elif AI_PROVIDER == "openai" and OPENAI_API_KEY:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {OPENAI_API_KEY}"
        }
        
        data = {
            "model": "gpt-4o-mini",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.2,
            "max_tokens": max_tokens
        }
        
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        else:
            raise Exception(f"OpenAI API error: {response.status_code}")
    
    else:
        raise Exception("No AI API key configured")




def remove_hashtags(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ í•´ì‹œíƒœê·¸ ì œê±°
    """
    import re
    # Remove hashtags (í•œê¸€/ì˜ë¬¸/ìˆ«ì)
    text = re.sub(r'#[\wê°€-í£]+', '', text)
    return text.strip()


# ì‚¬ìš©ì ì§€ì • í•µì‹¬ í‚¤ì›Œë“œ ëª©ë¡ (Jì—´ìš©)
CORE_KEYWORDS = [
    "ë³´ì–‘ëŒ€ì²©", "ê±´ê°•ë°±ì„œ", "ë°¥ì´ë³´ì•½", "ë“€ë¨¼", "ìˆ˜ì…",
    "ê°•ì•„ì§€", "ê³ ì–‘ì´", "ê¸°í˜¸", "ì†Œí™”", "ë³€",
    "ê¸°ë ¥", "í™œë ¥", "ì‹ìš•", "ì„¤ì‚¬", "ê±°ë¶€"
]

# ê²½ìŸì‚¬/ë¸Œëœë“œ ëª©ë¡ (Jì—´/Kì—´ìš©)
# í•œêµ­ ì£¼ìš” ì‚¬ë£Œ/ê°„ì‹ ë¸Œëœë“œ (ì•½ 50ê°œ)
COMPETITORS = [
    "ë³´ì–‘ëŒ€ì²©", "ë¡œì–„ìºë‹Œ", "íìŠ¤", "í“¨ë¦¬ë‚˜", "ë„¤ì¶”ëŸ´ì½”ì–´", 
    "ê±´ê°•ë°±ì„œ", "ë°¥ì´ë³´ì•½", "ë“€ë¨¼", "í•˜ë¦¼", "ë”ë¦¬ì–¼", 
    "ì‹œì €", "ANF", "ì˜¤ë¦¬ì  ", "ì•„ì¹´ë‚˜", "ì§€ìœ„í”½", 
    "K9", "ìŠ¤í…”ë¼ì•¤ì¸„ì´ìŠ¤", "ë¹…ë…", "ë‹¥í„°ë§˜ë§ˆ", "ë ˆì´ì•¤ì´ë³¸", 
    "ë„ê±°ë°•ìŠ¤", "ì›°ì¸ ", "ë‚˜ìš°", "ë‹¥í„°ë…", 
    "ëª½ìŠˆìŠˆ", "í˜ë””ê·¸ë¦¬", "í”„ë¡œí”Œëœ", "ì´ë‚˜ë°”", "í…œí…Œì´ì…˜", 
    "ìœ„ìŠ¤ì¹´ìŠ¤", "ì‰ë°”", "ì§í«", "ì¡°ê³µ", "ì‡ì¸„", 
    "í•í«", "êµ­ê°œëŒ€í‘œ", "ì•Œëª¨ë„¤ì´ì³", "ìœ”ì§€ìŠ¤", "ê·¸ë¦¬ë‹ˆì¦ˆ",
    "í¬ì¼„ìŠ¤", "ë¸Œë¦¬ì§€í…Œì¼", "í˜ìŠ¤ë£¸", "ë°”ì‡ë¯¸", "ë¦´ë¦¬ìŠ¤í‚¤ì¹œ",
    "ì„¸ë‹ˆë©”ë“œ", "ìŠ¤í˜ì‹œí”½", "ë²¨ë¦­ì„œ", "í•˜ì´í¬ì•ŒëŸ¬ì œë‹‰",
    # ì§§ì€ ë¸Œëœë“œëª… ë³´ì™„
    "ê³  ë„¤ì¶”ëŸ´", "ê³  ì‚¬ë£Œ", "Go! Solutions", "Go ì‚¬ë£Œ"
]


def extract_keywords_hybrid(title, content):
    """
    í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (Jì—´: í•µì‹¬ì—°ê´€í‚¤ì›Œë“œ)
    ì§€ì •ëœ í‚¤ì›Œë“œ ëª©ë¡ì—ì„œë§Œ ë§¤ì¹­
    
    Returns:
        str: ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ë¬¸ìì—´
    """
    found_keywords = []
    full_text = (title + " " + content[:1000])
    
    for keyword in CORE_KEYWORDS:
        if keyword in full_text:
            found_keywords.append(keyword)
    
    return ", ".join(found_keywords) if found_keywords else ""


def extract_brands_regex(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë¸Œëœë“œëª… ì¶”ì¶œ (Regex/List ê¸°ë°˜)
    """
    found_brands = set()
    for brand in COMPETITORS:
        if brand in text:
            found_brands.add(brand)
    return list(found_brands)

def merge_and_sort_brands(ai_brands_str, text):
    """
    AI ì¶”ì¶œ ë¸Œëœë“œì™€ Regex ì¶”ì¶œ ë¸Œëœë“œë¥¼ ë³‘í•©í•˜ê³  ì •ë ¬
    ê·œì¹™: 'ë³´ì–‘ëŒ€ì²©' ìµœìš°ì„ , ê·¸ ì™¸ì—ëŠ” ë°œê²¬ëœ ìˆœì„œ ë˜ëŠ” ê°€ë‚˜ë‹¤ìˆœ
    """
    # 1. Regexë¡œ í™•ì‹¤í•œ ë¸Œëœë“œ ì°¾ê¸°
    regex_brands = extract_brands_regex(text)
    
    # 2. AI ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    ai_brands = [b.strip() for b in ai_brands_str.split(',') if b.strip()]
    
    # 3. ë³‘í•© (Setìœ¼ë¡œ ì¤‘ë³µ ì œê±°)
    all_brands = set(regex_brands + ai_brands)
    
    # 4. ì •ë ¬
    sorted_brands = sorted(list(all_brands))
    
    # 5. ë³´ì–‘ëŒ€ì²© ìµœìš°ì„  ì²˜ë¦¬
    if "ë³´ì–‘ëŒ€ì²©" in sorted_brands:
        sorted_brands.remove("ë³´ì–‘ëŒ€ì²©")
        sorted_brands.insert(0, "ë³´ì–‘ëŒ€ì²©")
        
    return ", ".join(sorted_brands)

def extract_competitors(title, content):
    """(Deprecated) Legacy function, kept for compatibility if needed"""
    return merge_and_sort_brands("", title + " " + content)


def clean_ai_response(text):
    """
    AI ì‘ë‹µì—ì„œ ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸(**), ì´ëª¨ì§€ ë“± ì œê±°
    """
    import re
    # ** ë§ˆí¬ë‹¤ìš´ ì œê±°
    text = text.replace("**", "")
    text = text.replace("*", "")
    # ì´ëª¨ì§€ ì œê±° (ìœ ë‹ˆì½”ë“œ ì´ëª¨ì§€ ë²”ìœ„)
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # Emoticons
        u"\U0001F300-\U0001F5FF"  # Misc Symbols and Pictographs
        u"\U0001F680-\U0001F6FF"  # Transport and Map
        u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
        u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
        u"\U00002702-\U000027B0"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub('', text)
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def analyze_cafe_content(title, content):
    """
    ì¹´í˜ ê²Œì‹œê¸€ AI ìš”ì•½ (ë³¸ë¬¸ ìš”ì•½ë§Œ, í‚¤ì›Œë“œëŠ” ë³„ë„ í•¨ìˆ˜)
    
    Returns:
        dict: {"ìš”ì•½": "..."}
    """
    if not GEMINI_API_KEY and not OPENAI_API_KEY:
        # API ì—†ìœ¼ë©´ ë³¸ë¬¸ ì²« 100ì ë°˜í™˜
        clean_content = remove_hashtags(content)
        return {"ìš”ì•½": clean_content[:100] if clean_content else title[:100]}
    
    try:
        # í•´ì‹œíƒœê·¸ ì œê±°
        clean_title = remove_hashtags(title)
        clean_content = remove_hashtags(content)
        
        # ëª…í™•í•œ í”„ë¡¬í”„íŠ¸ (ë°˜ë ¤ë™ë¬¼ ì‚¬ë£Œ ê´€ë ¨ ìš”ì•½)
        # ëª…í™•í•œ í”„ë¡¬í”„íŠ¸ (ë°˜ë ¤ë™ë¬¼ ì‚¬ë£Œ ê´€ë ¨ ìš”ì•½)
        prompt = f"""ë°˜ë ¤ë™ë¬¼ ì‚¬ë£Œ ê´€ë ¨ ì¹´í˜ ê¸€ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì œëª©: {clean_title}
ë³¸ë¬¸: {clean_content[:500]}

ê·œì¹™:
1. ì´ ê¸€ì´ "ê°•ì•„ì§€" ë˜ëŠ” "ê³ ì–‘ì´"ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ê¸€ì¸ì§€ ê°€ì¥ ë¨¼ì € íŒë‹¨í•˜ì„¸ìš”. (ì†Œë¼ê²Œ, í–„ìŠ¤í„°, ì‚¬ëŒ ìŒì‹ ë“±ì€ False)
3. ì „ì²´ ë‚´ìš©ì„ 'ìŒìŠ´ì²´'(~í•¨, ~ì„)ë¡œ ëë‚˜ëŠ” ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„± (ê¶Œì¥ 100ì, ìµœëŒ€ 150ì)
4. ë§ˆí¬ë‹¤ìš´(**), ì´ëª¨ì§€, í•´ì‹œíƒœê·¸ ì‚¬ìš© ê¸ˆì§€
5. "ìš”ì•½:", "ê²°ë¡ :" ê°™ì€ ë¼ë²¨ ì—†ì´ ë°”ë¡œ ë‚´ìš©ë§Œ ì‘ì„±
6. 'ë¸Œëœë“œì–¸ê¸‰'ì—ëŠ” ë³¸ë¬¸ì— ì–¸ê¸‰ëœ ëª¨ë“  ì‚¬ë£Œ/ê°„ì‹ ë¸Œëœë“œëª…ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ ë‚˜ì—´í•˜ì„¸ìš”. ë‹¨, "ë³´ì–‘ëŒ€ì²©"ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ë§¨ ì²˜ìŒì— ì ìœ¼ì„¸ìš”. (ì˜ˆ: ë³´ì–‘ëŒ€ì²©, ë¡œì–„ìºë‹Œ, ê±´ê°•ë°±ì„œ)

ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ (ë‹¤ë¥¸ ë§ ì—†ì´ JSONë§Œ):
{{
  "ë°˜ë ¤ë™ë¬¼ê´€ë ¨": true ë˜ëŠ” false,
  "ìš”ì•½": "í•µì‹¬ ë‚´ìš© ìš”ì•½ (ìŒìŠ´ì²´)",
  "ë¸Œëœë“œì–¸ê¸‰": "ë³´ì–‘ëŒ€ì²©ì„ ìµœìš°ì„ ìœ¼ë¡œ í•œ ë¸Œëœë“œ ëª©ë¡ (ì—†ìœ¼ë©´ ë¹ˆì¹¸)"
}}"""

        ai_response = call_ai_api(prompt, max_tokens=200)
        
        # ë””ë²„ê¹…: AI ì›ë³¸ ì‘ë‹µ ì¶œë ¥
        print(f"      ğŸ“ AI ì›ë³¸ ì‘ë‹µ: {ai_response[:100]}...")
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            if "```" in ai_response:
                ai_response = ai_response.split("```")[1]
                if ai_response.startswith("json"):
                    ai_response = ai_response[4:]
            
            analysis = json.loads(ai_response)
            
            # ë§ˆí¬ë‹¤ìš´, ì´ëª¨ì§€ ì œê±° í›„ì²˜ë¦¬
            summary = clean_ai_response(analysis.get("ìš”ì•½", ""))[:150]
            is_relevant = analysis.get("ë°˜ë ¤ë™ë¬¼ê´€ë ¨", True)
            
            # ë¸Œëœë“œ ì–¸ê¸‰: AI ê²°ê³¼ + Regex ê²°ê³¼ ë³‘í•©
            ai_brand_mention = clean_ai_response(analysis.get("ë¸Œëœë“œì–¸ê¸‰", ""))
            final_brand_mention = merge_and_sort_brands(ai_brand_mention, title + " " + content)
            
            # ë¹ˆ ì‘ë‹µì´ë©´ í´ë°±
            if not summary or len(summary) < 10:
                print(f"      âš ï¸ AI ìš”ì•½ ë„ˆë¬´ ì§§ìŒ, ë³¸ë¬¸ìœ¼ë¡œ ëŒ€ì²´")
                summary = clean_content[:100] if clean_content else title[:100]
                
            return {
                "ìš”ì•½": summary, 
                "ë°˜ë ¤ë™ë¬¼ê´€ë ¨": is_relevant,
                "ë¸Œëœë“œì–¸ê¸‰": final_brand_mention
            }
            
        except Exception as json_err:
            print(f"      âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {json_err}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¼ë„ ê±´ì§€ê¸° ìœ„í•œ í´ë°±
            clean_text = clean_ai_response(ai_response)
            
            # í´ë°± ìƒí™©ì—ì„œë„ Regexë¡œ ë¸Œëœë“œ ì¶”ì¶œ ì‹œë„
            fallback_brands = extract_brands_regex(title + " " + content)
            fallback_brand_str = ", ".join(sorted(fallback_brands))
            
            return {
                "ìš”ì•½": clean_text[:100], 
                "ë°˜ë ¤ë™ë¬¼ê´€ë ¨": True,
                "ë¸Œëœë“œì–¸ê¸‰": fallback_brand_str
            }
    
    except Exception as e:
        print(f"      âš ï¸ AI ìš”ì•½ ì‹¤íŒ¨: {e}")
        clean_content = remove_hashtags(content)
        return {"ìš”ì•½": clean_content[:100] if clean_content else title[:100]}



def analyze_daily_summary(blog_rows, cafe_rows):
    """
    ì¼ì¼ ìˆ˜ì§‘ ë°ì´í„° í†µí•© ë¶„ì„ (ì „ë¬¸ê°€ ëª¨ë“œ)
    
    Args:
        blog_rows: ë¸”ë¡œê·¸ ìˆ˜ì§‘ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        cafe_rows: ì¹´í˜ ìˆ˜ì§‘ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        
    Returns:
        str: ì „ë¬¸ê°€ ë¶„ì„ ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸
    """
    if not GEMINI_API_KEY and not OPENAI_API_KEY:
        return "AI APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ í†µí•© ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ë°ì´í„° ìš”ì•½
    total_count = len(blog_rows) + len(cafe_rows)
    if total_count == 0:
        return "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
    # ì œëª©ê³¼ ìš”ì•½ë§Œ ì¶”ì¶œí•´ì„œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    content_summary = "ã€ë¸”ë¡œê·¸ ë°ì´í„°ã€‘\n"
    for row in blog_rows[:15]:  # í† í° ì œí•œ ê³ ë ¤ ìƒìœ„ 15ê°œ
        content_summary += f"- {row[2]} (ìš”ì•½: {row[5]})\n"
        
    content_summary += "\nã€ì¹´í˜ ë°ì´í„°ã€‘\n"
    for row in cafe_rows[:15]:  # í† í° ì œí•œ ê³ ë ¤ ìƒìœ„ 15ê°œ
        content_summary += f"- {row[3]} (ìš”ì•½: {row[6]})\n"
        
    prompt = f"""ë‹¹ì‹ ì€ ë°˜ë ¤ë™ë¬¼ ì‹í’ˆ ë¸Œëœë“œ 'ë³´ì–‘ëŒ€ì²©'ì˜ ë§ˆì¼€íŒ… ì „ëµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ì™€ ì¹´í˜ì˜ 'ì¸ê¸° ê²Œì‹œê¸€(ê´€ë ¨ë„ìˆœ)' ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì „ëµì„ ì œì•ˆí•˜ì„¸ìš”.

[ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½]
{content_summary}

---
[ë¶„ì„ ìš”êµ¬ì‚¬í•­]
ë‹¤ìŒ 3ê°€ì§€ ê´€ì ì—ì„œ ì˜ˆë¦¬í•˜ê²Œ ë¶„ì„í•˜ì—¬ ë³´ê³ í•´ì£¼ì„¸ìš”. (ì¡´ëŒ“ë§, ê° í•­ëª©ë³„ 2~3ë¬¸ì¥)

1. ğŸ—£ï¸ ì†Œë¹„ì ë°˜ì‘ (Consumer Voice)
   - ì†Œë¹„ìë“¤ì´ ëŠë¼ëŠ” ë‚ ê²ƒì˜ ê°ì •ì´ë‚˜ ë¶ˆí¸í•¨ì€ ë¬´ì—‡ì¸ê°€?
   - ë³´ì–‘ëŒ€ì²© íŒë§¤ìê°€ ë†“ì¹˜ì§€ ë§ì•„ì•¼ í•  'ì•¡ê¸°ìŠ¤' ì •ë³´ëŠ”?

2. ğŸ­ ì‹œì¥ íŠ¸ë Œë“œ & ì œì¡°ì‚¬ ì „ëµ (Market & Manufacturer)
   - í˜„ì¬ ì‹œì¥ì˜ íë¦„ì´ë‚˜ ê²½ìŸì‚¬ë“¤ì˜ ì›€ì§ì„ì—ì„œ í¬ì°©ëœ íŒ¨í„´ì€?
   - ì†Œë¹„ìë“¤ì˜ ì‹¬ë¦¬ì  ë³€í™”ë‚˜ ìƒˆë¡œìš´ ë‹ˆì¦ˆëŠ” ë¬´ì—‡ì¸ê°€?

3. ğŸš€ ë³´ì–‘ëŒ€ì²© ë§ˆì¼€íŒ… ì „ëµ (Action Plan)
   - ì˜¤ëŠ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ê°€?
   - ì–´ë–»ê²Œ ì‹œì¥ì„ íŒŒê³ ë“¤ì–´ ì„±ì¥ì„ ë§Œë“¤ì–´ë‚¼ ê²ƒì¸ê°€? (êµ¬ì²´ì ì´ê³  ì‹¤í˜„ ê°€ëŠ¥í•œ ì œì•ˆ)

[ì¶œë ¥ í˜•ì‹]
## ğŸ“Š ì˜¤ëŠ˜ì˜ ì „ë¬¸ê°€ ë¶„ì„ ë¦¬í¬íŠ¸

1. ğŸ—£ï¸ **ì†Œë¹„ì ë°˜ì‘**
(ë‚´ìš©)

2. ğŸ­ **ì‹œì¥ íŠ¸ë Œë“œ**
(ë‚´ìš©)

3. ğŸš€ **ë³´ì–‘ëŒ€ì²© ì „ëµ**
(ë‚´ìš©)"""

    try:
        return call_ai_api(prompt, max_tokens=1000)
    except Exception as e:
        return f"í†µí•© ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}"


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("=== í˜‘ì°¬ ê°ì§€ í…ŒìŠ¤íŠ¸ ===")
    test1 = detect_sponsored_content(
        "[í˜‘ì°¬] ë³´ì–‘ëŒ€ì²© ì›Œë° í›„ê¸°",
        "ì´ë²ˆì— í˜‘ì°¬ë°›ì•„ ì‚¬ìš©í•´ë´¤ì–´ìš”. ì •ë§ ì¢‹ë„¤ìš”!"
    )
    print(f"í˜‘ì°¬ê¸€ ê°ì§€: {test1}")  # True
    
    print("\n=== ì§ˆë¬¸ê¸€ íŒë³„ í…ŒìŠ¤íŠ¸ ===")
    test2 = is_genuine_question(
        "ë³´ì–‘ëŒ€ì²© ì–´ë–¤ê°€ìš”?",
        "ìš°ë¦¬ ê°•ì•„ì§€í•œí…Œ ë¨¹ì—¬ë„ ê´œì°®ì„ê¹Œìš”? ì•ŒëŸ¬ì§€ê°€ ìˆëŠ”ë°..."
    )
    print(f"ì§ˆë¬¸ê¸€ íŒë³„: {test2}")  # True
    
    print("\n=== ëŒ“ê¸€ ê°ì„± ë¶„ì„ í…ŒìŠ¤íŠ¸ ===")
    test3 = analyze_comment_sentiment("ìš°ë¦¬ ì•„ì´ëŠ” ì„¤ì‚¬ê°€ ë‚˜ì™”ì–´ìš”...")
    print(f"ê°ì„±: {test3}")  # ë¶€ì •
