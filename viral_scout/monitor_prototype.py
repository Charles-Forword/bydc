import time
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# ------------------------------------------------------
# ğŸ•µï¸â€â™‚ï¸ Viral Scout: Community Monitor (MVP)
# ëª©í‘œ: íŠ¹ì • ì»¤ë®¤ë‹ˆí‹°ì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ìƒˆ ê¸€ì„ ë°œê²¬í•˜ë©´ ì•Œë¦¼(ì¶œë ¥)
# ------------------------------------------------------

# ì„¤ì •: ëª¨ë‹ˆí„°ë§í•  í‚¤ì›Œë“œì™€ ëŒ€ìƒ URL
# ì˜ˆì‹œë¡œ ë„¤ì´ë²„ ì¹´í˜ëŠ” ì ‘ê·¼ ì œì–´ê°€ ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ, 
# í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì ‘ê·¼ì´ ì‰¬ìš´ ê³µê°œ ì»¤ë®¤ë‹ˆí‹°(ì˜ˆ: ë””ì‹œì¸ì‚¬ì´ë“œ ë©ë©ì´ ê°¤ëŸ¬ë¦¬ ë“±)ë‚˜
# í˜¹ì€ ê°€ìƒì˜ í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿì„ ìƒì •í•˜ê³  ì‘ì„±í•©ë‹ˆë‹¤.
# ì‹¤ì œ ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ì€ ë¡œê·¸ì¸ ì„¸ì…˜ ë“± ì¶”ê°€ ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.

TARGET_URL = "https://gall.dcinside.com/board/lists/?id=dog" # ì˜ˆì‹œ: ë©ë©ì´ ê°¤ëŸ¬ë¦¬
SEARCH_KEYWORDS = ["ì‚¬ë£Œ", "ë°¥", "ì¶”ì²œ", "ì•ˆë¨¹ì–´", "ë³´ì–‘ëŒ€ì²©"] 
CHECK_INTERVAL_SECONDS = 60 # 1ë¶„ë§ˆë‹¤ í™•ì¸

def fetch_latest_posts():
    """ì»¤ë®¤ë‹ˆí‹°ì˜ ìµœì‹  ê¸€ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(TARGET_URL, headers=headers)
        if response.status_code == 200:
            return response.text
        else:
            print(f"Error: {response.status_code}")
            return None
    except Exception as e:
        print(f"Connection Error: {e}")
        return None

def parse_posts(html):
    """HTMLì—ì„œ ê¸€ ì œëª©ê³¼ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ë””ì‹œì¸ì‚¬ì´ë“œ ì˜ˆì‹œ)"""
    soup = BeautifulSoup(html, 'html.parser')
    posts = []
    
    # ë””ì‹œì¸ì‚¬ì´ë“œ ê°¤ëŸ¬ë¦¬ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ì— ë§ì¶˜ íŒŒì‹± (ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¦„)
    # ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” ëŒ€ìƒ ì‚¬ì´íŠ¸ì˜ HTML êµ¬ì¡° ë¶„ì„ì´ ì„ í–‰ë˜ì–´ì•¼ í•¨
    rows = soup.select('tr.ub-content')
    
    for row in rows:
        try:
            title_tag = row.select_one('.gall_tit a')
            if title_tag:
                title = title_tag.text.strip()
                link = "https://gall.dcinside.com" + title_tag['href']
                posts.append({'title': title, 'link': link})
        except AttributeError:
            continue
            
    return posts

def monitor():
    print(f"ğŸ•µï¸â€â™‚ï¸ Viral Scout ê°€ë™ ì‹œì‘... (íƒ€ê²Ÿ: {TARGET_URL})")
    print(f"ğŸ” ê°ì‹œ í‚¤ì›Œë“œ: {SEARCH_KEYWORDS}")
    
    seen_posts = set() # ì´ë¯¸ ë³¸ ê¸€ ì¤‘ë³µ ë°©ì§€

    while True:
        html = fetch_latest_posts()
        if html:
            posts = parse_posts(html)
            new_sightings = 0
            
            for post in posts:
                post_id = post['link'] # ë§í¬ë¥¼ ê³ ìœ  IDë¡œ ì‚¬ìš©
                
                if post_id not in seen_posts:
                    seen_posts.add(post_id)
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                    for keyword in SEARCH_KEYWORDS:
                        if keyword in post['title']:
                            print(f"\n[ğŸš¨ í¬ì°©ë¨!] í‚¤ì›Œë“œ '{keyword}' ë°œê²¬")
                            print(f"ì œëª©: {post['title']}")
                            print(f"ë§í¬: {post['link']}")
                            print("-" * 30)
                            new_sightings += 1
                            break # í•œ ê¸€ì— ì—¬ëŸ¬ í‚¤ì›Œë“œê°€ ìˆì–´ë„ í•œ ë²ˆë§Œ ì•Œë¦¼
            
            if new_sightings == 0:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] íŠ¹ì´ì‚¬í•­ ì—†ìŒ...", end='\r')
        
        time.sleep(CHECK_INTERVAL_SECONDS)

if __name__ == "__main__":
    monitor()
